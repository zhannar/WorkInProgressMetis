{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What needs to be done (general plan):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On: http://www.digital.nyc/startups\n",
    "- http://www.digital.nyc/startups\n",
    "- http://www.digital.nyc/startups?page=1\n",
    "- http://www.digital.nyc/startups?page=380\n",
    "\n",
    "# Out of each page, extract out:\n",
    "- Startup name : Next Island LLC\n",
    "- Website on the digital thing: http://www.digital.nyc/startups/next-island-llc\n",
    "\n",
    "# Go to the website: e.g. \n",
    "- http://www.digital.nyc/startups/next-island-llc\n",
    "\n",
    "# Pull the \"Script\" via beautiful soup:\n",
    "- <script>jQuery.extend(Drupal.settings, ...\n",
    "\n",
    "# Select out the lat & long values\n",
    "\"center\":{\"lat\":\"\",\"lon\":\"\",\"geocode\":\"Find my location\"}},\"data\":{\"type\":\"Point\",\"coordinates\":[-74.006,40.7144],\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing the work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://www.boxofficemojo.com/intl/china/yearly/?yr=2015&p=.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_location_data(url):\n",
    "    response = requests.get(url)\n",
    "    page_html = response.text\n",
    "    soup = BeautifulSoup(page_html)\n",
    "    \n",
    "    # Retrieve all of the script tags\n",
    "    blah = soup('tbody')\n",
    "    print blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_location_data(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the list of startup pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digital_nyc_startups_pages_list = [\"http://www.digital.nyc/startups\"]\n",
    "\n",
    "for i in range(1,380):\n",
    "    website = \"http://www.digital.nyc/startups?page=\" + str(i)\n",
    "    digital_nyc_startups_pages_list.append(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.digital.nyc/startups', 'http://www.digital.nyc/startups?page=1', 'http://www.digital.nyc/startups?page=2']\n"
     ]
    }
   ],
   "source": [
    "print digital_nyc_startups_pages_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go through all those http://www.digital.nyc/startups/ links and compile a list of individual companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_company_links(startup_url):\n",
    "    '''This takes some webpage like http://www.digital.nyc/startups?page=1\n",
    "    and returns a list of tuples (company_data_listof the form (company name, company url).\n",
    "    That object is called companies_list'''\n",
    "    response = requests.get(startup_url)\n",
    "    page_html = response.text\n",
    "    soup = BeautifulSoup(page_html)\n",
    "    # Retrieve all of the link tags\n",
    "    links_collection = soup.find_all('a')\n",
    "    companies_list = []\n",
    "    for link in links_collection:\n",
    "        try:\n",
    "            # There are some links without href -- this keeps the code from \n",
    "            # breaking when it gets to them\n",
    "            #print link[\"href\"]\n",
    "            if \"/startups/\" in link[\"href\"]:    #This selects only htose links of startups\n",
    "                #print True\n",
    "                company_url = \"http://www.digital.nyc/\" + str(link[\"href\"])\n",
    "                #print company_url\n",
    "                company_name = link[\"href\"][10:]\n",
    "                #print company_name\n",
    "                company_tuple = (company_name,company_url)\n",
    "                if company_tuple not in companies_list:\n",
    "                    companies_list.append(company_tuple)\n",
    "        except:\n",
    "            pass\n",
    "    return companies_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_list_of_company_websites = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning the next step is going to take a really long time (it actually scrapes the web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for webpage in digital_nyc_startups_pages_list:\n",
    "    #print webpage\n",
    "    #print get_company_links(fwebpage)\n",
    "    big_list_of_company_websites = big_list_of_company_websites + get_company_links(webpage)\n",
    "    #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7494\n"
     ]
    }
   ],
   "source": [
    "print len(big_list_of_company_websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('genesis', 'http://www.digital.nyc//startups/genesis'),\n",
       " ('yank-technologies-inc',\n",
       "  'http://www.digital.nyc//startups/yank-technologies-inc'),\n",
       " ('cinematcher', 'http://www.digital.nyc//startups/cinematcher'),\n",
       " ('1-atelier-llc', 'http://www.digital.nyc//startups/1-atelier-llc'),\n",
       " ('first-wall-street', 'http://www.digital.nyc//startups/first-wall-street'),\n",
       " ('httpwwwbluemailmediacom',\n",
       "  'http://www.digital.nyc//startups/httpwwwbluemailmediacom'),\n",
       " ('studytree', 'http://www.digital.nyc//startups/studytree'),\n",
       " ('heed-media', 'http://www.digital.nyc//startups/heed-media'),\n",
       " ('pips-island', 'http://www.digital.nyc//startups/pips-island'),\n",
       " ('raleigh-and-drake', 'http://www.digital.nyc//startups/raleigh-and-drake')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list_of_company_websites[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving it to csv just in case...\n",
    "import csv\n",
    "\n",
    "with open('big_list_of_company_websites.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(big_list_of_company_websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_list_of_company_websites1 = big_list_of_company_websites[0:1000]\n",
    "big_list_of_company_websites2 = big_list_of_company_websites[1000:2000]\n",
    "big_list_of_company_websites3 = big_list_of_company_websites[2000:3000]\n",
    "big_list_of_company_websites4 = big_list_of_company_websites[3000:4000]\n",
    "big_list_of_company_websites5 = big_list_of_company_websites[4000:5000]\n",
    "big_list_of_company_websites6 = big_list_of_company_websites[5000:6000]\n",
    "big_list_of_company_websites7 = big_list_of_company_websites[6000:7000]\n",
    "big_list_of_company_websites8 = big_list_of_company_websites[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goes to Website, pulls out script, extracts out coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_location_data(url):\n",
    "    response = requests.get(url)\n",
    "    page_html = response.text\n",
    "    soup = BeautifulSoup(page_html)\n",
    "    \n",
    "    # Retrieve all of the script tags\n",
    "    scripts_collection = soup('script')\n",
    "    # Finds script associated with the google maps thing in the top right\n",
    "    google_script_BSTag = scripts_collection[8]\n",
    "    # This is a bunch of stuff to make the script easy to work with\n",
    "    google_script_string = str(google_script_BSTag)\n",
    "    google_script_string_dict_portion = google_script_string[39:len(google_script_string)-11]    # Removes the javascript/jquery grammer to just get the JSON portion\n",
    "\n",
    "    google_script_dict = json.loads(google_script_string_dict_portion)\n",
    "    google_script_dict['geofieldMap']\n",
    "\n",
    "    #pp = pprint.PrettyPrinter(depth=6)\n",
    "    #pp.pprint(google_script_dict)\n",
    "    # (There was a bunch of steps here nec. to determine where the lat long info was contained)\n",
    "\n",
    "    \n",
    "    # http://stackoverflow.com/questions/17106819/accessing-python-dict-values-with-the-key-start-characters\n",
    "    # It seems the key that looks like the following changes it's numbers ever time there's a new point. Since\n",
    "    # I can't predict the numbers, I'll just have create the key on the fly.\n",
    "    # [\"geofield-map-entity-node-965-field-geo-location\"]\n",
    "\n",
    "    geofield_etc_key = google_script_dict['geofieldMap'].keys()[0]\n",
    "    lat_long_list = google_script_dict['geofieldMap'][geofield_etc_key][\"data\"][\"coordinates\"]\n",
    "    return lat_long_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-74.006, 40.7144]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_location_data(\"http://www.digital.nyc/startups/next-island-llc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-74.0059, 40.7128]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_location_data(\"http://www.digital.nyc//startups/cinematcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Commenting this out just to make sure peopel don't accidently erase the data\n",
    "full_list_company_data = []\n",
    "full_list_company_data1 = []\n",
    "full_list_company_data2 = []\n",
    "full_list_company_data3 = []\n",
    "full_list_company_data4 = []\n",
    "full_list_company_data5 = []\n",
    "full_list_company_data6 = []\n",
    "full_list_company_data7 = []\n",
    "full_list_company_data8 = []\n",
    "'''\n",
    "# Estimated it may take 30-1hr\n",
    "# #1 - Started at 11:43 -- done by 12:16\n",
    "# #2 - Started at 12:45 -- done by 1:07\n",
    "# #3 - Started at 1:08 -- done by \n",
    "# #4 - Started at  -- done by \n",
    "# #5 - Started at  -- done by \n",
    "# #6 - Started at  -- done by \n",
    "# #7 - Started at  -- done by \n",
    "# #8 - Started at  -- done by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for company_name_and_link in big_list_of_company_websites4:\n",
    "    name = company_name_and_link[0]\n",
    "    link = company_name_and_link[1]\n",
    "    try:\n",
    "        location = extract_location_data(link)\n",
    "        longitude = location[0]\n",
    "        latitude = location[1]\n",
    "        company_tuple_full = (name,link,latitude,longitude)\n",
    "        full_list_company_data4.append(company_tuple_full)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Websites to Scrape: 1000 Total Companies w/ scraped Lat/Long Data 939\n",
      "Total Websites to Scrape: 1000 Total Companies w/ scraped Lat/Long Data 980\n",
      "Total Websites to Scrape: 1000 Total Companies w/ scraped Lat/Long Data 913\n",
      "Total Websites to Scrape: 1000 Total Companies w/ scraped Lat/Long Data 0\n",
      "Total Websites to Scrape: 1000 Total Companies w/ scraped Lat/Long Data 0\n",
      "Total Websites to Scrape: 1000 Total Companies w/ scraped Lat/Long Data 0\n",
      "Total Websites to Scrape: 1000 Total Companies w/ scraped Lat/Long Data 0\n",
      "Total Websites to Scrape: 494 Total Companies w/ scraped Lat/Long Data 0\n"
     ]
    }
   ],
   "source": [
    "print \"Total Websites to Scrape:\", len(big_list_of_company_websites1), \"Total Companies w/ scraped Lat/Long Data\", len(full_list_company_data1)\n",
    "print \"Total Websites to Scrape:\", len(big_list_of_company_websites2), \"Total Companies w/ scraped Lat/Long Data\", len(full_list_company_data2)\n",
    "print \"Total Websites to Scrape:\", len(big_list_of_company_websites3), \"Total Companies w/ scraped Lat/Long Data\", len(full_list_company_data3)\n",
    "print \"Total Websites to Scrape:\", len(big_list_of_company_websites4), \"Total Companies w/ scraped Lat/Long Data\", len(full_list_company_data4)\n",
    "print \"Total Websites to Scrape:\", len(big_list_of_company_websites5), \"Total Companies w/ scraped Lat/Long Data\", len(full_list_company_data5)\n",
    "print \"Total Websites to Scrape:\", len(big_list_of_company_websites6), \"Total Companies w/ scraped Lat/Long Data\", len(full_list_company_data6)\n",
    "print \"Total Websites to Scrape:\", len(big_list_of_company_websites7), \"Total Companies w/ scraped Lat/Long Data\", len(full_list_company_data7)\n",
    "print \"Total Websites to Scrape:\", len(big_list_of_company_websites8), \"Total Companies w/ scraped Lat/Long Data\", len(full_list_company_data8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('genesis', 'http://www.digital.nyc//startups/genesis', 40.7152, -73.7422),\n",
       " ('yank-technologies-inc',\n",
       "  'http://www.digital.nyc//startups/yank-technologies-inc',\n",
       "  40.7128,\n",
       "  -74.0059),\n",
       " ('cinematcher',\n",
       "  'http://www.digital.nyc//startups/cinematcher',\n",
       "  40.7128,\n",
       "  -74.0059),\n",
       " ('1-atelier-llc',\n",
       "  'http://www.digital.nyc//startups/1-atelier-llc',\n",
       "  40.7545,\n",
       "  -73.9944),\n",
       " ('first-wall-street',\n",
       "  'http://www.digital.nyc//startups/first-wall-street',\n",
       "  40.7128,\n",
       "  -74.0059),\n",
       " ('httpwwwbluemailmediacom',\n",
       "  'http://www.digital.nyc//startups/httpwwwbluemailmediacom',\n",
       "  40.7128,\n",
       "  -74.0059),\n",
       " ('studytree',\n",
       "  'http://www.digital.nyc//startups/studytree',\n",
       "  40.7128,\n",
       "  -74.0059),\n",
       " ('heed-media',\n",
       "  'http://www.digital.nyc//startups/heed-media',\n",
       "  40.753,\n",
       "  -73.9842),\n",
       " ('pips-island',\n",
       "  'http://www.digital.nyc//startups/pips-island',\n",
       "  40.7436,\n",
       "  -73.9933),\n",
       " ('raleigh-and-drake',\n",
       "  'http://www.digital.nyc//startups/raleigh-and-drake',\n",
       "  40.7128,\n",
       "  -74.0059)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_list_company_data1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
