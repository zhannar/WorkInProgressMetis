{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair Problem\n",
    "\n",
    "Practice Lasso regularization technique in four steps on the given data set:\n",
    "\n",
    "1) Use the KFold function from sklearn to divide the data into 5 training/test sets.\n",
    "\n",
    "2) Tune the lambda parameter in the lasso model by looping over a grid of possible lambdas (sklearn: ridge)\n",
    "\n",
    "For each candidate lambda, loop over the 5 training/test sets.  \n",
    "On each training/test set run the lasso model on the training set and then compute and record the prediction error in the test set.  \n",
    "Finally total the prediction error for the 5 training/test sets.\n",
    "3) Set lambda to be the value that minimizes prediction error.\n",
    "\n",
    "4) Run the lasso model again with the optimal lambda determined in step 3. Which variables would you consider excluding on the basis of these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# >>> import numpy as np\n",
    "# >>> from sklearn.cross_validation import KFold\n",
    "\n",
    "# >>> kf = KFold(20, n_folds=4)\n",
    "# >>> for train, test in kf:\n",
    "#     print(\"%s %s\" % (train, test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Lasso_practice_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 21)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                x1           x2           x3           x4           x5  \\\n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
      "mean     -0.011640     0.027039    -0.016687    -0.017795     0.007121   \n",
      "std       1.012714     0.995109     1.006237     1.004680     1.015677   \n",
      "min      -3.488603    -3.900697    -4.326393    -3.091918    -3.182018   \n",
      "25%      -0.699153    -0.639319    -0.708722    -0.677776    -0.662261   \n",
      "50%      -0.016876     0.013312    -0.019923    -0.015773     0.006291   \n",
      "75%       0.677067     0.714541     0.672212     0.659879     0.699014   \n",
      "max       3.025285     4.354047     2.956734     4.352949     3.312204   \n",
      "\n",
      "                x6           x7           x8           x9          x10  \\\n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
      "mean      0.020547     0.012657    -0.018621     0.014379     0.001625   \n",
      "std       0.998204     0.987151     0.984812     0.985713     1.034154   \n",
      "min      -3.161528    -3.552433    -3.278217    -3.078226    -3.243556   \n",
      "25%      -0.663618    -0.666011    -0.709522    -0.637123    -0.716359   \n",
      "50%       0.035863     0.015729    -0.014469    -0.014229    -0.010495   \n",
      "75%       0.681367     0.664726     0.650502     0.660233     0.694403   \n",
      "max       3.740365     3.869983     3.127594     3.586890     3.313268   \n",
      "\n",
      "          ...               x12          x13          x14          x15  \\\n",
      "count     ...       2000.000000  2000.000000  2000.000000  2000.000000   \n",
      "mean      ...          0.012533    -0.011359    -0.020866    -0.023228   \n",
      "std       ...          0.981842     1.005848     1.007256     0.994412   \n",
      "min       ...         -3.405717    -3.187674    -3.369315    -3.703379   \n",
      "25%       ...         -0.626243    -0.708768    -0.695575    -0.687240   \n",
      "50%       ...         -0.010137    -0.009152    -0.039288    -0.000749   \n",
      "75%       ...          0.657211     0.697336     0.661021     0.664318   \n",
      "max       ...          3.341589     3.379833     3.131566     3.673774   \n",
      "\n",
      "               x16          x17          x18          x19          x20  \\\n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
      "mean     -0.001987    -0.011329    -0.034003    -0.028972    -0.006286   \n",
      "std       1.008890     1.011755     0.985786     1.012087     1.006900   \n",
      "min      -3.213464    -3.375419    -3.828841    -3.441899    -3.780123   \n",
      "25%      -0.701671    -0.687095    -0.683281    -0.710962    -0.700564   \n",
      "50%      -0.025660     0.021768    -0.012918    -0.020121     0.000814   \n",
      "75%       0.665662     0.676909     0.607443     0.655827     0.700879   \n",
      "max       3.090694     3.780511     3.026105     2.988361     3.170770   \n",
      "\n",
      "                 y  \n",
      "count  2000.000000  \n",
      "mean     -0.105200  \n",
      "std       3.757293  \n",
      "min     -13.188121  \n",
      "25%      -2.594692  \n",
      "50%      -0.155337  \n",
      "75%       2.459879  \n",
      "max      12.666420  \n",
      "\n",
      "[8 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note!!! It is very important that you convert it into a numpy array and not enter it as a pandas array\n",
    "y = np.array(data[\"y\"])\n",
    "X = np.array(data.drop(\"y\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kf_scrap = KFold(15, n_folds=5)\n",
    "# for train_index, test_index in kf_scrap:\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)  \n",
    "#     #print(\"%s %s\" % (train_index, test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # This function will take a sample with a certain number of elements (the first parameter), and\n",
    "# # and break it into the number of folds specified, with that many subsets as well.\n",
    "\n",
    "# kf_scrap = KFold(20, n_folds=5)\n",
    "# #, shuffle=False,random_state=None)\n",
    "# print len(kf_scrap)   # This is the number of folds and iterations\n",
    "\n",
    "# for train_index, test_index in kf_scrap:\n",
    "#     print(\"%s %s\" % (train_index, test_index))\n",
    "#     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     #print(\"%s %s\" % (train_index.shape, test_index.shape))\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "#     # After this step is where we do the fitting and etc...\n",
    "#     # For each fold, train on the training sets (X_train & y_train)\n",
    "    \n",
    "    \n",
    "#     # For each fold, get a cost for the test sets (X_test & y_test)\n",
    "\n",
    "    \n",
    "#     # Store the cost in a separate cost list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 20)\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n = 2000, n_folds=5)\n",
    "#, shuffle=True,random_state=0)\n",
    "print X.shape\n",
    "print len(kf)\n",
    "\n",
    "#lambda_range = np.arange(0,2,0.001)    # Ideal to start with a course grid and then make it finer. E.g. (0,1,0.05) -> (0,0.05, 0.001)\n",
    "lambda_range = np.arange(0,0.033,0.0001)    # Ideal to start with a course grid and then make it finer. E.g. (0,1,0.05) -> (0,0.05, 0.001)\n",
    "\n",
    "error_per_lambda = []\n",
    "#print lambda_range\n",
    "\n",
    "for lambda_value in lambda_range:\n",
    "    errors_per_fold_list = []\n",
    "    for train_index, test_index in kf:\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        #print(\"%s %s\" % (train_index.shape, test_index.shape))\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # After this step is where we do the fitting and etc...\n",
    "        # For each fold, train on the training sets (X_train & y_train)\n",
    "        model = Lasso(lambda_value)    # The number entered here is the lambda /alpha variable\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        # For each fold, get a cost for the test sets (X_test & y_test)\n",
    "        y_hat = model.predict(X_test)      # this will generate the predictions\n",
    "        errors = y_test - y_hat\n",
    "        squared_errors = errors * errors\n",
    "        sum_squared_errors = sum(squared_errors)\n",
    "   \n",
    "    #   Returns the coefficient of determination R^2 of the prediction.\n",
    "    #   The coefficient R^2 is defined as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n",
    "\n",
    "        #r_squared = model.score(X_test,y_test)    # R_squared = 1 - (SSE/SST)\n",
    "        #sse_divied_sst = 1 - r_squared\n",
    "\n",
    "        # Store the cost in a separate cost list\n",
    "\n",
    "        errors_per_fold_list.append(sum_squared_errors)\n",
    "        # print errors_per_fold_list\n",
    "        # Compute the average cost across all folds for the given lambda\n",
    "\n",
    "    final_lambda_cost = np.mean(errors_per_fold_list)\n",
    "\n",
    "    # Store the cost of the lambda\n",
    "\n",
    "    error_per_lambda.append(final_lambda_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Alpha/Lambda Value  Mean Error per Fold\n",
      "151              0.0151           404.992063\n",
      "150              0.0150           404.992282\n",
      "149              0.0149           404.992611\n",
      "148              0.0148           404.993050\n",
      "152              0.0152           404.993363\n",
      "147              0.0147           404.993600\n",
      "146              0.0146           404.994260\n",
      "153              0.0153           404.994771\n",
      "145              0.0145           404.995037\n",
      "144              0.0144           404.995924\n",
      "154              0.0154           404.996284\n",
      "143              0.0143           404.996639\n",
      "142              0.0142           404.997147\n",
      "141              0.0141           404.997766\n",
      "155              0.0155           404.997905\n",
      "140              0.0140           404.998497\n",
      "139              0.0139           404.999340\n",
      "156              0.0156           404.999635\n",
      "138              0.0138           405.000294\n",
      "157              0.0157           405.000714\n"
     ]
    }
   ],
   "source": [
    "lambda_error_df = pd.DataFrame({\"Alpha/Lambda Value\":lambda_range,\"Mean Error per Fold\": error_per_lambda}).sort(columns = \"Mean Error per Fold\")\n",
    "print lambda_error_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.         -1.78806882 -0.11457123  0.         -0.          1.83363665\n",
      "  0.          0.         -0.18350592  0.15521782 -0.         -0.         -0.\n",
      " -2.19425204  0.          0.99649116  0.03126941 -0.0025321   0.         -0.3317956 ]\n"
     ]
    }
   ],
   "source": [
    "best_lambda =  0.0243\n",
    "model_final = Lasso(best_lambda)\n",
    "model_final.fit(X,y)\n",
    "print model_final.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
